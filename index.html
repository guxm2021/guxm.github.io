<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiangming Gu</title>
  
  <meta name="author" content="Xiangming Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiangming Gu</name>
              </p>
              <p>I am a second-year Ph.D. student from <a href="https://smcnus.github.io">NUS Sound and Music Computing Lab</a>, where I am supervised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Wang Ye</a>. I am affilated to <a href="https://isep.nus.edu.sg">Integrative Sciences and Engineering Programme</a> and <a href="https://www.comp.nus.edu.sg">School of Computing</a> at <a href="https://www.nus.edu.sg">National University of Singapore</a>.
              </p>
              <p>
                I obtained my B.E. degree of Electronic Engineering and B.S. degree of Finance at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. My undergraduate thesis was supervised by Prof. <a href="http://jschenthu.weebly.com">Jiansheng Chen</a> and the topic was about techniques and applications of 3D Human Pose Estimation in Heathcare.
              </p>
              <p>
                My research interests include Machine Learning, mainly on unsupervised learning and transfer learning; Computer Vision, mainly on 3D Human Pose Estimation; Music Information Retrieval, etc.
              </p>
              <p style="text-align:center">
                <a href="mailto:xiangming@comp.nus.edu.sg">Email</a> &nbsp/&nbsp
                <a href="pdf/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiangming-gu/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/gu_xiangming">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/guxm2021">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XiangmingGu.JPG"><img style="width:50%;max-width:50%" alt="profile photo" src="images/XiangmingGu.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>

          <ul>
            <li>
              <strong>[2022.10]</strong>: Our paper MM-ALT was selected as the top 13 rated paper in (<i>ACM MM</i>) 2022!
          </li>
            <li>
              <strong>[2022.09]</strong>: One paper got accepted to Advances in Neural Information Processing Systems (<i>NeurIPS</i>) 2022!
          </li>
            <li>
              <strong>[2022.07]</strong>: One paper got accepted to International Society for Music Information Retrieval Conference (<i>ISMIR</i>) 2022!
          </li>
            <!-- <li>
              <strong>[2022.07]</strong>: <i>NUS Sound and Music Computing Lab</i> has survived from a server attack and has a <a href="https://smcnus.github.io">brand new lab website</a> now!
          </li> -->
            <li>
                <strong>[2022.06]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>ACM MM</i>) 2022!
            </li>
            <li>
              <strong>[2022.05]</strong>: One paper got accepted to IEEE Transactions on Image Processing (<i>TIP</i>), 2022!
          </li>
          <li>
            <strong>[2021.08]</strong>: I started my Ph.D. Journey at National University of Singapore!
        </li>
          </ul>
          </td>

        </tr>
      </tbody></table>

      <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Preprints</heading>

          </td>

        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/arxiv2022.png" width="160" height="160">
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.02670.pdf">
                <papertitle>Unsupervised Mismatch Localization in Cross-Modal Sequential Data</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>*,
              <a href="https://scholar.google.com/citations?user=GQm1eZEAAAAJ&hl=zh-CN">Hengguan Huang</a>*,
              <strong>Xiangming Gu</strong>,
              <a href="http://www.wanghao.in">Hao Wang</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              <em>Preprint</em>, 2022.
              <br>
              <a href="pdf/Arxiv2022.pdf">pdf</a> /
            <a href="data/wei2022unsupervised.bib">bibtex</a>		
              <p></p>
              <p>We developed an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences.</p>
            </td>
          </tr>
        </tbody></table>
        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>

            </td>

          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><u>2022</u></heading>

          </td>

        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips2022.png" width="160" height="137">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</papertitle>
                <br>
                <a href="https://scholar.google.com/citations?user=GQm1eZEAAAAJ&hl=zh-CN">Hengguan Huang</a>,
                <strong>Xiangming Gu</strong>,
                <a href="http://www.wanghao.in">Hao Wang</a>,
                <a href="https://smcnus.github.io">Chang Xiao</a>,
                <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=6xFZDEcAAAAJ">Hongfu Liu</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                Advances in Neural Information Processing Systems (<em>NeurIPS</em>), 2022.
                <br>
                pdf /
                bibtex /
                code
                <p></p>
                <p>We proposed to formulate predictive modeling as a continuous-time Bayesian filtering problem and extrapolative continuous-time Bayesian neural networks (ECBNN).</p>
              </td>
            </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ismir2022.png" width="160" height="160">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2207.09747.pdf">
                  <papertitle>Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/longshen-ou/">Longshen Ou</a>*,
                <strong>Xiangming Gu</strong>*,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                International Society for Music Information Retrieval Conference (<em>ISMIR</em>), 2022. 
                <br>
                <a href="pdf/ISMIR2022.pdf">pdf</a> /
                <a href="data/ou2022towards.bib">bibtex</a> /
                <a href="https://github.com/guxm2021/ALT_SpeechBrain">code</a>
                <p></p>
                <p>We proposed a transfer-learning-based Automatic Lyric Transcription solution and achieved state-of-the-art performances on benchmark datasets.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acmmm2022.png" width="160" height="160">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">
                  <papertitle>MM-ALT: A Multimodal Automatic Lyric Transcription System</papertitle>
                </a>
                <br>
                <strong>Xiangming Gu</strong>*,
                <a href="https://www.linkedin.com/in/longshen-ou/">Longshen Ou</a>*,
                <a href="https://www.linkedin.com/in/danielle-ong-854b88177/">Danielle Ong</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                ACM International Conference on Multimedia (<em>ACM MM</em>), 2022. (<font color="red"><i>Oral, Top 13 papers</i></font>)
                <br>
                <a href="pdf/ACMMM2022.pdf">pdf</a> /
                <a href="data/gu2022mm.bib">bibtex</a> /
                <a href="https://n20em.github.io">project page</a> /
                <a href="https://github.com/guxm2021/MM_ALT">code</a>
                <p></p>
                <p>We proposed a MultiModal Automatic Lyric Transcription System accepting audio, video and IMU modalities and also curated the N20EM dataset.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tip2022.png" width="160" height="160">
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9798770">
                  <papertitle>Boosting Monocular 3D Human Pose Estimation with Part Aware Attention</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=NksnxhwAAAAJ&hl=zh-CN">Youze Xue</a>, 
                <a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ&hl=zh-CN">Jiansheng Chen</a>,  
                <strong>Xiangming Gu</strong>,
                <a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ&hl=en">Huimin Ma</a>,
                <a href="http://web.ee.tsinghua.edu.cn/mahongbing/en/index.htm">Hongbing Ma</a>,
                <br>
                 IEEE Transactions on Image Processing (<em>TIP</em>), 2022. (<font color="red"><i>Impact factor: 11.041</i></font>)
                <br>
                <a href="https://ieeexplore.ieee.org/document/9798770">IEEE paper</a> /
                <a href="data/xue2022boost.bib">bibtex</a> /
                <a href="https://github.com/thuxyz19/3D-HPE-PAA">code</a>
                <p></p>
                <p>We proposed part aware attention mechanism which helps a transformer-based model to achieve state-of-the-art 3D pose estimation performance.</p>
              </td>
            </tr>
          </tbody></table>
					
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><u>2021</u></heading>
  
            </td>
  
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/iros2021.png" width="160" height="160">
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">
                <papertitle>Laser Endoscopic Manipulator Using Spring-Reinforced Multi-DoF Soft Actuator</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
              <a href="https://www.researchgate.net/scientific-contributions/Penghui-Yang-2149548367">Penghui Yang</a>,  
              <strong>Xiangming Gu</strong>,
              <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>
              <br>
               IEEE/RSJ International Conference on Intelligent Robots and Systems (<em>IROS</em>), 2021. 
               <br>
               Also in IEEE Robotics and Automation Letter (<em>RA-L</em>), 2021. (<font color="red"><i>Impact factor: 4.321</i></font>)
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">IEEE paper</a> /
              <a href="data/zhang2021laser.bib">bibtex</a>		
              <p></p>
              <p>We developed a layer endoscopic manipulator with a soft bendable tip for minimally invasive surgery.</p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><u>2020</u></heading>
  
              </td>
  
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/arxiv2020.png" width="160" height="160">
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2010.04974.pdf">
                    <papertitle>Distilling a Deep Neural Network into a Takagi-Sugeno-Kang Fuzzy Inference System</papertitle>
                  </a>
                  <br>
                  <strong>Xiangming Gu</strong>,
                  <a href="https://online.ece.nus.edu.sg/staff/web.asp?id=elexc">Xiang Cheng</a>
                  <br>
                  <em>Technical Report</em>, 2020.
                  <br>
                  <a href="pdf/Arxiv2020.pdf">pdf</a> /
                  <a href="data/gu2020distilling.bib">bibtex</a>		
                  <p></p>
                  <p>We distilled knowledge from deep learning models into explainable TSK-type Fussy Inference systems.</p>
                </td>
              </tr>
  
          </tbody></table>
        
          <HR>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nus_logo.jpg" alt="nus">
            </td>
            <td width="75%" valign="center">
              Teaching Assistant, <a href="https://github.com/xbresson/CS4243_2022">CS4243, Computer Vision and Pattern Recognition, Spring 2022</a>
              Teaching Assistant, <a href="https://knmnyn.github.io/cs3244-2210">CS3244, Machine Learning, Fall 2022</a>
              <br>
            </td>
          </tr>
					

        </tbody></table>

        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>.
                <br>
                Last Updated September 2022.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
