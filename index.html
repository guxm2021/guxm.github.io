<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiangming Gu</title>
  
  <meta name="author" content="Xiangming Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiangming Gu</name>
              </p>
              <p>I am a second-year Ph.D. candidate from <a href="https://smcnus.github.io">NUS Sound and Music Computing Lab</a>, where I am supervised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Wang Ye</a>. I am affilated to <a href="https://isep.nus.edu.sg">Integrative Sciences and Engineering Programme</a> and <a href="https://www.comp.nus.edu.sg">School of Computing</a> at <a href="https://www.nus.edu.sg">National University of Singapore</a>. Before that, I obtained my B.E. degree of Electronic Engineering and B.S. degree of Finance at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021.
              </p>
              <p>
                My research interests include Machine Learning, mainly on unsupervised learning and transfer learning; 3D Vision; Music Information Retrieval, etc.
              </p>
              <p style="text-align:center">
                <a href="mailto:xiangming@comp.nus.edu.sg">Email</a> &nbsp/&nbsp
                <a href="pdf/Xiangming_Gu_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiangming-gu/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/gu_xiangming">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/guxm2021">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XiangmingGu.JPG"><img style="width:50%;max-width:50%" alt="profile photo" src="images/XiangmingGu.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>

          <ul>
          <li>
              <strong>[2022.12]</strong>: One paper got accepted to Transactions on Machine Learning Research (<i>TMLR</i>'2022)!
          </li>
          <li>
              <strong>[2022.12]</strong>: I passed my Ph.D. Qualifying Examination (PQE) and became a Ph.D. candidate!
          </li>
          <li>
            <strong>[2022.11]</strong>: I physically attended <i>NeurIPS</i>'2022 in New Orleans, United States!
          </li>
          <li>
            <strong>[2022.10]</strong>: I physically attended <i>MM</i>'2022 in Lisbon, Portugal, where our paper <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">MM-ALT</a> won the <strong>Top Paper Award</strong>!
          </li>
            <li>
              <strong>[2022.09]</strong>: One paper got accepted to Advances in Neural Information Processing Systems (<i>NeurIPS</i>'2022)!
          </li>
            <li>
              <strong>[2022.07]</strong>: One paper got accepted to International Society for Music Information Retrieval Conference (<i>ISMIR</i>'2022)!
          </li>
            <li>
                <strong>[2022.06]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>MM</i>'2022)!
            </li>
            <li>
              <strong>[2022.05]</strong>: One paper got accepted to IEEE Transactions on Image Processing (<i>TIP</i>'2022)!
          </li>
          </ul>
          </td>

        </tr>
      </tbody></table>

      <!-- <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Preprints</heading>

          </td>

        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.02670.pdf">
                <papertitle>Unsupervised Mismatch Localization in Cross-Modal Sequential Data</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>*,
              <a href="https://scholar.google.com/citations?user=GQm1eZEAAAAJ&hl=zh-CN">Hengguan Huang</a>*,
              <strong>Xiangming Gu</strong>,
              <a href="http://www.wanghao.in">Hao Wang</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              <em>Transactions on Machine Learning Research</em>, 2023.
              <br>
              <a href="pdf/Arxiv2022.pdf">pdf</a> /
            <a href="data/wei2022unsupervised.bib">bibtex</a>		
              <p></p>
              <p>We proposed mismatch localization variational autoencoder (ML-VAE) as well as a novel and effective training procedure, which is an unsupervised learning algorithm, to infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences.</p>
            </td>
          </tr>
        </tbody></table> -->

        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>

            </td>

          </tr>
        </tbody></table>
        

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><u>2023</u></heading>

          </td>

        </tr>
      </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading><u>2022</u></heading>

          </td>

        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/arxiv2022.png" width="160" height="160">
          </td> -->
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href=" https://openreview.net/forum?id=29V0xo7jKp">
                <papertitle>Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>*,
              <a href="https://scholar.google.com/citations?user=GQm1eZEAAAAJ&hl=zh-CN">Hengguan Huang</a>*,
              <strong>Xiangming Gu</strong>,
              <a href="http://www.wanghao.in">Hao Wang</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              Transactions on Machine Learning Research (<em>TMLR</em>'2022).
              <br>
              <a href="pdf/TMLR2022.pdf">pdf</a> /
              <a href="data/wei2022unsupervised.bib">bibtex</a>
              <a href="https://github.com/weiwei-ww/ML-VAE">code</a>
              <p></p>
              <p>We proposed mismatch localization variational autoencoder (ML-VAE) as well as a novel and effective training procedure, which is an unsupervised learning algorithm, to infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences.</p>
            </td>
          </tr>

      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips2022.png" width="160" height="137">
            </td> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=wiHzQWwg3l">
                  <papertitle>Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=GQm1eZEAAAAJ&hl=zh-CN">Hengguan Huang</a>,
                <strong>Xiangming Gu</strong>,
                <a href="http://www.wanghao.in">Hao Wang</a>,
                <a href="https://smcnus.github.io">Chang Xiao</a>,
                <a href="https://waffle-liu.github.io">Hongfu Liu</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                Advances in Neural Information Processing Systems (<em>NeurIPS</em>'2022).
                <br>
                <a href="pdf/NeurIPS2022.pdf">pdf</a> /
                <a href="data/huang2022.bib">bibtex</a> /
                <a href="https://github.com/guxm2021/ECBNN">code /
                <a href="https://recorder-v3.slideslive.com/?share=76177&s=cd597422-3109-42f3-9072-dfef5c351a4d">video</a>
                <p></p>
                <p>We proposed to incorporate the mechanism of internel preditive modeling into UDA algorithms to handle non-stationary streaming data and achieve training-free test-time adaptation with low latency. Our solution extrapolative continuous-time Bayesian neural network (ECBNN) exceeds UDA baselines on both synthetic and real-world data.</p>
              </td>
            </tr>
        
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ismir2022.png" width="160" height="160">
            </td> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2207.09747.pdf">
                  <papertitle>Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</papertitle>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/longshen-ou/">Longshen Ou</a>*,
                <strong>Xiangming Gu</strong>*,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                International Society for Music Information Retrieval Conference (<em>ISMIR</em>'2022). 
                <br>
                <a href="pdf/ISMIR2022.pdf">pdf</a> /
                <a href="data/ou2022towards.bib">bibtex</a> /
                <a href="https://github.com/guxm2021/ALT_SpeechBrain">code</a>
                <p></p>
                <p>We proposed to transfer the self-supervised-learning models, e.x. wav2vec 2.0 from the speech domain to singing domain. Our built Automatic Lyric Transcription system achieved state-of-the-art performances on various benchmark datasets.</p>
              </td>
            </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acmmm2022.png" width="160" height="160">
            </td> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">
                  <papertitle>MM-ALT: A Multimodal Automatic Lyric Transcription System</papertitle>
                </a>
                <br>
                <strong>Xiangming Gu</strong>*,
                <a href="https://www.linkedin.com/in/longshen-ou/">Longshen Ou</a>*,
                <a href="https://www.linkedin.com/in/danielle-ong-854b88177/">Danielle Ong</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                ACM International Conference on Multimedia (<em>MM</em>'2022). (<font color="red"><i>Oral, Top Paper Award</i></font>)
                <br>
                <a href="pdf/ACMMM2022.pdf">pdf</a> /
                <a href="pdf/ACMMM2022_Appendix.pdf">appendix</a> /
                <a href="data/gu2022mm.bib">bibtex</a> /
                <a href="https://n20em.github.io">project page</a> /
                <a href="https://github.com/guxm2021/MM_ALT">code</a> /
                <a href="https://youtu.be/3eA62RYROq0">video</a>
                <p></p>
                <p>We proposed the problem setting of MultiModal Automatic Lyric Transcription (ALT) as well as its solution: MM-ALT which transcribes the lyrics from synchronized audio, video and IMU modalities. To evaluate MM-ALT, we also curated the first multimodal ALT dataset: N20EM.</p>
              </td>
            </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tip2022.png" width="160" height="160">
            </td> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9798770">
                  <papertitle>Boosting Monocular 3D Human Pose Estimation with Part Aware Attention</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=NksnxhwAAAAJ&hl=zh-CN">Youze Xue</a>, 
                <a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ&hl=zh-CN">Jiansheng Chen</a>,  
                <strong>Xiangming Gu</strong>,
                <a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ&hl=en">Huimin Ma</a>,
                <a href="http://web.ee.tsinghua.edu.cn/mahongbing/en/index.htm">Hongbing Ma</a>,
                <br>
                 IEEE Transactions on Image Processing (<em>TIP</em>'2022). (<font color="red"><i>Impact factor: 11.041</i></font>)
                <br>
                <a href="https://ieeexplore.ieee.org/document/9798770">IEEE paper</a> /
                <a href="data/xue2022boost.bib">bibtex</a> /
                <a href="https://github.com/thuxyz19/3D-HPE-PAA">code</a>
                <p></p>
                <p>We proposed part aware temporal attention and part aware dictionary attention to help a transformer-based model to achieve the state-of-the-art 3D pose estimation performance on various benchmark datasets.</p>
              </td>
            </tr>
          </tbody></table>
					
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><u>2021 and Before</u></heading>
  
            </td>
  
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <!-- <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/iros2021.png" width="160" height="160">
          </td> -->
            <td style="padding:20px;width:100%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">
                <papertitle>Laser Endoscopic Manipulator Using Spring-Reinforced Multi-DoF Soft Actuator</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
              <a href="https://www.researchgate.net/scientific-contributions/Penghui-Yang-2149548367">Penghui Yang</a>,  
              <strong>Xiangming Gu</strong>,
              <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>
              <br>
               IEEE/RSJ International Conference on Intelligent Robots and Systems (<em>IROS</em>'2021). 
               <br>
               Also in IEEE Robotics and Automation Letter (<em>RA-L</em>'2021). (<font color="red"><i>Impact factor: 4.321</i></font>)
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">IEEE paper</a> /
              <a href="data/zhang2021laser.bib">bibtex</a>		
              <p></p>
              <p>We designed a new spring-reinforced Mutl-DoF soft actuator and used it to develop a layer endoscopic manipulator with a soft bendable tip for minimally invasive surgery.</p>
            </td>
          </tr>

        </tbody></table>


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><u>2020</u></heading>
  
              </td>
  
            </tr>
          </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/arxiv2020.png" width="160" height="160">
              </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2010.04974.pdf">
                    <papertitle>Distilling a Deep Neural Network into a Takagi-Sugeno-Kang Fuzzy Inference System</papertitle>
                  </a>
                  <br>
                  <strong>Xiangming Gu</strong>,
                  <a href="https://online.ece.nus.edu.sg/staff/web.asp?id=elexc">Xiang Cheng</a>
                  <br>
                  <em>Technical Report</em>, 2020.
                  <br>
                  <a href="pdf/Arxiv2020.pdf">pdf</a> /
                  <a href="data/gu2020distilling.bib">bibtex</a>		
                  <p></p>
                  <p>We proposed to distill the knowledge from deep learning models into TSK-type Fussy Inference systems, which improves the performance of interpretable models.</p>
                </td>
              </tr>
  
          </tbody></table>
        
          <HR>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nus_logo.jpg" alt="nus">
            </td> -->
            <td width="100%" valign="center">
              Teaching Assistant, <a href="https://knmnyn.github.io/cs3244-2210">CS3244, Machine Learning, Fall 2022</a>
              <br>
              Teaching Assistant, <a href="https://github.com/xbresson/CS4243_2022">CS4243, Computer Vision and Pattern Recognition, Spring 2022</a>
              <br>
            </td>
          </tr>
					

        </tbody></table>

        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>.
                <br>
                Last Updated December 2022.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
